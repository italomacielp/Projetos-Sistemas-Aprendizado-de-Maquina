{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#AirBnB Price Prediction with Machine Learning\n",
        "##Componentes\n",
        "- Ítalo Maciel\n",
        "- Paula Souza"
      ],
      "metadata": {
        "id": "Bt63o9Lyjzb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download and Unzip Dataset"
      ],
      "metadata": {
        "id": "dG0vgs0xjw7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "9k0-9KnWlDld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76xzo0pOjEZZ"
      },
      "outputs": [],
      "source": [
        "!wget -O listings.csv.gz \"https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2025-03-19/data/listings.csv.gz\"\n",
        "\n",
        "!gunzip -k listings.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ],
      "metadata": {
        "id": "segDGoR7qzto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from torch.utils.data.dataset import random_split\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "id": "5U1yQDa_qyfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read csv and create dataset"
      ],
      "metadata": {
        "id": "NXBHXGwFmsAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df = pd.read_csv(\"listings.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "isi0gsXlksbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create attributes to the model"
      ],
      "metadata": {
        "id": "cvP6jsRDnIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_columns = [\n",
        "    \"accommodates\",      # Number of guests the property can host\n",
        "    \"bathrooms\",         # Number of bathrooms available\n",
        "    \"bedrooms\",          # Number of bedrooms available\n",
        "    \"beds\",              # Number of beds available\n",
        "    \"minimum_nights\",    # Minimum nights required for a booking\n",
        "    \"maximum_nights\",    # Maximum nights allowed for a booking\n",
        "    \"number_of_reviews\", # Total reviews given by past guests\n",
        "    \"neighbourhood_cleansed\",\n",
        "    \"property_type\", # How a guest books space\n",
        "    \"amenities\", # Basic items\n",
        "    \"review_scores_rating\",\n",
        "    \"estimated_occupancy_l365d\",\n",
        "    \"price\"              # Nightly rental price\n",
        "]\n",
        "\n",
        "rio_listings = df[target_columns].copy()\n",
        "\n",
        "rio_listings.sample(5)"
      ],
      "metadata": {
        "id": "KujP2qstl0QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_listings.describe()"
      ],
      "metadata": {
        "id": "vIpffJiUsUJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_listings.info()"
      ],
      "metadata": {
        "id": "6diPuYGPsYFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "xAsmuj6Bp6Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stripped_commas = rio_listings['price'].str.replace(',', '')\n",
        "\n",
        "stripped_dollars = stripped_commas.str.replace('$', '')\n",
        "\n",
        "rio_listings['price'] = stripped_dollars.astype('float')"
      ],
      "metadata": {
        "id": "7aaE3hTNsccB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_listings.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "eAjkVQfe4kV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordem = rio_listings['property_type'].astype('category').unique().tolist()\n",
        "rio_listings['property_type_cod'] = pd.Categorical(rio_listings['property_type'], categories=ordem, ordered=True)\n",
        "\n",
        "rio_listings['property_type_cod'] = rio_listings['property_type_cod'].cat.codes\n",
        "\n",
        "ordem = rio_listings['neighbourhood_cleansed'].astype('category').unique().tolist()\n",
        "rio_listings['neighbourhood_cleansed_cod'] = pd.Categorical(rio_listings['neighbourhood_cleansed'], categories=ordem, ordered=True)\n",
        "\n",
        "rio_listings['neighbourhood_cleansed_cod'] = rio_listings['neighbourhood_cleansed_cod'].cat.codes\n",
        "\n",
        "ordem = rio_listings['amenities'].astype('category').unique().tolist()\n",
        "rio_listings['amenities_cod'] = pd.Categorical(rio_listings['amenities'], categories=ordem, ordered=True)\n",
        "\n",
        "rio_listings['amenities_cod'] = rio_listings['amenities_cod'].cat.codes\n"
      ],
      "metadata": {
        "id": "exg7UpxipHgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_listings = rio_listings.drop(['neighbourhood_cleansed', 'property_type', 'amenities'], axis=1)"
      ],
      "metadata": {
        "id": "gvjMXxUVixYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application of the outliers"
      ],
      "metadata": {
        "id": "US6TuRzBqTpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _iqr_bounds(series: pd.Series, k: float = 1.5):\n",
        "\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    low = q1 - k * iqr\n",
        "    up = q3 + k * iqr\n",
        "    return low, up, {\"Q1\": q1, \"Q3\": q3, \"IQR\": iqr}\n",
        "\n",
        "\n",
        "def remove_outliers_iqr(\n",
        "    df: pd.DataFrame,\n",
        "    columns: list[str],\n",
        "    k: float = 1.5,\n",
        "    inclusive: bool = True,\n",
        "    dropna: bool = True,\n",
        "):\n",
        "\n",
        "    data = df.copy()\n",
        "\n",
        "    for c in columns:\n",
        "        data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
        "\n",
        "    if dropna:\n",
        "        data = data.dropna(subset=columns)\n",
        "\n",
        "    before = len(data)\n",
        "    bounds = {}\n",
        "\n",
        "    mask = pd.Series(True, index=data.index)\n",
        "\n",
        "    for c in columns:\n",
        "        low, up, stats = _iqr_bounds(data[c].dropna(), k=k)\n",
        "        bounds[c] = {\"low\": low, \"up\": up, **stats}\n",
        "\n",
        "        if inclusive:\n",
        "            m = (data[c] >= low) & (data[c] <= up)\n",
        "        else:\n",
        "            m = (data[c] > low) & (data[c] < up)\n",
        "\n",
        "        mask &= m\n",
        "\n",
        "    cleaned = data.loc[mask].copy()\n",
        "    info = {\n",
        "        \"rows_in\": before,\n",
        "        \"rows_out\": len(cleaned),\n",
        "        \"rows_removed\": before - len(cleaned),\n",
        "        \"k\": k,\n",
        "        \"inclusive\": inclusive,\n",
        "        \"bounds\": bounds,\n",
        "    }\n",
        "    return cleaned, info"
      ],
      "metadata": {
        "id": "UyTCzclv7u4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_iqr_input = rio_listings[list(rio_listings.columns)].copy()\n",
        "\n",
        "# Remove outliers with default Tukey rule (k=1.5) and inclusive bounds\n",
        "rio_iqr, summary = remove_outliers_iqr(\n",
        "    df=rio_iqr_input,\n",
        "    columns=list(rio_listings.columns),\n",
        "    k=1.5,\n",
        "    inclusive=True,\n",
        "    dropna=True,\n",
        ")\n",
        "\n",
        "print(f\"Rows before:  {summary['rows_in']}\")\n",
        "print(f\"Rows after:   {summary['rows_out']}\")\n",
        "print(f\"Removed:      {summary['rows_removed']}\")\n",
        "print(\"Per-column bounds (low/up):\")\n",
        "for col, b in summary[\"bounds\"].items():\n",
        "    print(f\"  - {col}: [{b['low']:.3f}, {b['up']:.3f}]  (Q1={b['Q1']:.3f}, Q3={b['Q3']:.3f}, IQR={b['IQR']:.3f})\")\n",
        "\n",
        "rio_iqr.head()"
      ],
      "metadata": {
        "id": "_INYLv7mjkU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_iqr.price.describe()"
      ],
      "metadata": {
        "id": "uTYxrat6kGaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation of features with target **Price**"
      ],
      "metadata": {
        "id": "6DmAyAMBqjqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute pairwise correlation matrix using Pearson's correlation coefficient\n",
        "corr_matrix = rio_iqr.corr(method=\"pearson\")\n",
        "\n",
        "# Display features sorted by correlation strength with the target ('price')\n",
        "# This helps identify which features are most relevant to predict 'price'\n",
        "price_corr = corr_matrix[\"price\"].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation of features with target 'price':\\n\")\n",
        "print(price_corr)"
      ],
      "metadata": {
        "id": "L1ztCHxlkNx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize correlation matrix\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,          # show correlation values\n",
        "    fmt=\".2f\",           # format with 2 decimals\n",
        "    cmap=\"coolwarm\",     # colormap: negative=blue, positive=red\n",
        "    center=0,            # center color scale at 0\n",
        "    square=True,         # square cells\n",
        "    cbar_kws={\"shrink\": 0.75}  # adjust colorbar size\n",
        ")\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title(\"Correlation Heatmap of Rio Listings Features\", fontsize=16, pad=15)\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x5DhKdSpkSik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename,weights_only=False)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "oazkU7m4kdOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_iqr.info()"
      ],
      "metadata": {
        "id": "V_n87HtVki6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build linear regression with Pytorch"
      ],
      "metadata": {
        "id": "3uKpVBfVs5wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1) Extract features (X) and target (y) from the DataFrame\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Keep all numeric feature columns except the target 'price'\n",
        "feature_cols = [c for c in rio_iqr.columns if c != \"price\"]\n",
        "target_col   = \"price\"\n",
        "\n",
        "# Convert to NumPy arrays (float32 is ideal for PyTorch)\n",
        "X = rio_iqr[feature_cols].to_numpy(dtype=np.float32)     # shape (N, D)\n",
        "y = rio_iqr[target_col].to_numpy(dtype=np.float32).reshape(-1, 1)  # shape (N, 1)\n",
        "\n",
        "# Quick sanity checks\n",
        "assert not np.isnan(X).any(), \"Found NaNs in X. Clean/impute before training.\"\n",
        "assert not np.isnan(y).any(), \"Found NaNs in y. Clean/impute before training.\""
      ],
      "metadata": {
        "id": "aHCCiU4vkkid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 2) Build tensors BEFORE splitting (as you requested)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(13)\n",
        "\n",
        "x_tensor = torch.as_tensor(X).float()   # (N, D)\n",
        "y_tensor = torch.as_tensor(y).float()   # (N, 1)\n",
        "\n",
        "# Whole dataset\n",
        "dataset = TensorDataset(x_tensor, y_tensor)"
      ],
      "metadata": {
        "id": "R0ZERDWxkpEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3) Train/validation split using PyTorch's random_split\n",
        "# ---------------------------------------------------------------------\n",
        "ratio = 0.8\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * ratio)\n",
        "n_val   = n_total - n_train\n",
        "\n",
        "train_data, val_data = random_split(dataset, [n_train, n_val])"
      ],
      "metadata": {
        "id": "dpF47rN_krGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization with Z-Score"
      ],
      "metadata": {
        "id": "hgcOTojStGwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Z-score without leakage\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# 0) Get split indices produced by random_split (already done above)\n",
        "train_idx = train_data.indices\n",
        "val_idx   = val_data.indices\n",
        "\n",
        "# 1) Compute mean/std ONLY on training subset\n",
        "eps = 1e-8\n",
        "mu  = x_tensor[train_idx].mean(dim=0)\n",
        "std = x_tensor[train_idx].std(dim=0, unbiased=False)\n",
        "std = torch.where(std < eps, torch.ones_like(std), std)  # avoid divide-by-zero\n",
        "\n",
        "y_mu  = y_tensor[train_idx].mean(dim=0)\n",
        "y_std = y_tensor[train_idx].std(dim=0, unbiased=False)\n",
        "y_std = torch.where(y_std < eps, torch.ones_like(y_std), y_std)\n",
        "\n",
        "\n",
        "# 2) Apply z-score to ALL features using training stats\n",
        "x_tensor_z = (x_tensor - mu) / std\n",
        "y_tensor_z = (y_tensor - y_mu) / y_std\n",
        "\n",
        "\n",
        "# 3) Rebuild dataset with normalized features and REUSE the SAME indices\n",
        "dataset_z  = TensorDataset(x_tensor_z, y_tensor_z)\n",
        "train_data = Subset(dataset_z, train_idx)\n",
        "val_data   = Subset(dataset_z, val_idx)"
      ],
      "metadata": {
        "id": "lqNnGk7bktm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization with Min-Max"
      ],
      "metadata": {
        "id": "Zoeyz41EtyqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Índices de treino e validação\n",
        "train_idx = train_data.indices\n",
        "val_idx   = val_data.indices\n",
        "\n",
        "# 1) Calcula min/max apenas no conjunto de treino\n",
        "x_train = x_tensor[train_idx]\n",
        "y_train = y_tensor[train_idx]\n",
        "\n",
        "x_min = x_train.min(dim=0).values\n",
        "x_max = x_train.max(dim=0).values\n",
        "\n",
        "y_min = y_train.min(dim=0).values\n",
        "y_max = y_train.max(dim=0).values\n",
        "\n",
        "# Evita divisão por zero\n",
        "eps = 1e-8\n",
        "x_range = torch.where((x_max - x_min) < eps, torch.ones_like(x_max), x_max - x_min)\n",
        "y_range = torch.where((y_max - y_min) < eps, torch.ones_like(y_max), y_max - y_min)\n",
        "\n",
        "# 2) Aplica Min-Max Scaling em todo o dataset usando estatísticas do treino\n",
        "x_tensor_scaled = (x_tensor - x_min) / x_range\n",
        "y_tensor_scaled = (y_tensor - y_min) / y_range\n",
        "\n",
        "# 3) Reconstrói os conjuntos com os mesmos índices\n",
        "dataset_scaled = TensorDataset(x_tensor_scaled, y_tensor_scaled)\n",
        "train_data = Subset(dataset_scaled, train_idx)\n",
        "val_data   = Subset(dataset_scaled, val_idx)"
      ],
      "metadata": {
        "id": "uUtzyZFJtuog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization with Log"
      ],
      "metadata": {
        "id": "5x4myGoat_yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Índices de treino e validação\n",
        "train_idx = train_data.indices\n",
        "val_idx   = val_data.indices\n",
        "\n",
        "# 1) Evita valores negativos ou zero antes de aplicar log\n",
        "eps = 1e-8  # pequeno valor para evitar log(0)\n",
        "x_tensor_log = torch.log(x_tensor + 1.0 + eps)\n",
        "y_tensor_log = torch.log(y_tensor + 1.0 + eps)\n",
        "\n",
        "# 2) Reconstrói o dataset com os mesmos índices\n",
        "dataset_log = TensorDataset(x_tensor_log, y_tensor_log)\n",
        "train_data  = Subset(dataset_log, train_idx)\n",
        "val_data    = Subset(dataset_log, val_idx)"
      ],
      "metadata": {
        "id": "vDA04HwWuCpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Recreate the DataLoaders\n",
        "batch_size  = 16\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(dataset=val_data,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Quick peek to confirm shapes\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"Train batch X: {xb.shape} | y: {yb.shape}\")  # e.g., (16, D) and (16, 1)"
      ],
      "metadata": {
        "id": "e4FhGxlwkvux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) (Optional but safer) Make model input dimension dynamic\n",
        "# Sets learning rate\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "D = x_tensor.shape[1]  # number of features\n",
        "model = nn.Sequential(nn.Linear(D, 1))\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss(reduction='mean')"
      ],
      "metadata": {
        "id": "mjHJgjgGkxo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "arch = Architecture(model, loss_fn, optimizer)\n",
        "arch.set_seed(42)\n",
        "arch.set_loaders(train_loader, val_loader)\n",
        "arch.train(n_epochs=n_epochs)"
      ],
      "metadata": {
        "id": "ctBaj5vQk1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = arch.plot_losses()"
      ],
      "metadata": {
        "id": "_oCtz36hlWVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_iqr.info()"
      ],
      "metadata": {
        "id": "ZBd6MxgxlW0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with line of the dataset"
      ],
      "metadata": {
        "id": "4Ka5TZRQmdXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array([[4,\t1.0,\t1.0,\t1.0,\t1,\t365,\t1,\t4.00,\t6, 0,\t0,\t18868]], dtype=np.float32)\n",
        "# -------------------------------------------------------------\n",
        "# 1) Apply the SAME feature normalization (z-score using training mu/std)\n",
        "# -------------------------------------------------------------\n",
        "X_new_t = torch.as_tensor(X_new)\n",
        "X_new_norm = (X_new_t - mu) / std  # mu, std from TRAIN only\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Predict with your trained Architecture\n",
        "# -------------------------------------------------------------\n",
        "y_pred_z = arch.predict(X_new_norm.numpy())   # prediction in standardized space of y\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Revert target normalization back to original units\n",
        "# -------------------------------------------------------------\n",
        "y_pred_real = y_pred_z * y_std.item() + y_mu.item()\n",
        "\n",
        "# Convert to scalar\n",
        "y_pred_real_value = float(y_pred_real.squeeze())\n",
        "\n",
        "print(f\"Predicted price: R$ {y_pred_real_value:.2f}\")"
      ],
      "metadata": {
        "id": "Ndwd3oAVlYcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparation with lib LazyPrediction"
      ],
      "metadata": {
        "id": "B1WCKAI9kVyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = rio_iqr.drop(\"price\", axis=1)\n",
        "y = rio_iqr[\"price\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "linear_reg_model = LinearRegression()\n",
        "linear_reg_model.fit(X_train, y_train)\n",
        "y_pred = linear_reg_model.predict(X_test)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.title('Actual vs Predicted Prices (Linear Regression)', fontsize=16)\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "1Asp43XAklmZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
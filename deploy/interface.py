# -*- coding: utf-8 -*-
"""Interface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vm1kcWAXCWv5dNeFN41pzBZksmXF3NMm

# Estalações e download da database
"""

!wget -O listings.csv.gz "https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2025-03-19/data/listings.csv.gz"

!gunzip -k listings.csv.gz

!pip install gradio matplotlib seaborn pandas scikit-learn lazypredict

"""# **Bibliotecas**"""

# Commented out IPython magic to ensure Python compatibility.
import gradio as gr
import seaborn as sns
import pandas as pd
from os import name
import matplotlib.pyplot as plt
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from lazypredict.Supervised import LazyRegressor
from torch.utils.data import TensorDataset, DataLoader, Subset, random_split
import torch.nn as nn
import torch.optim as optim
# %matplotlib inline
plt.style.use('fivethirtyeight')
import numpy as np
import datetime
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.utils.data.dataset import random_split

pd.set_option('display.max_columns', None)

df = pd.read_csv("listings.csv")
df.head()

target_columns = [
    "accommodates",      # Number of guests the property can host
    "bathrooms",         # Number of bathrooms available
    "bedrooms",          # Number of bedrooms available
    "beds",              # Number of beds available
    "minimum_nights",    # Minimum nights required for a booking
    "maximum_nights",    # Maximum nights allowed for a booking
    "number_of_reviews", # Total reviews given by past guests
    "neighbourhood_cleansed", #
    "property_type", # How a guest books space
    "amenities", # Basic items
    "review_scores_rating", #
    "estimated_occupancy_l365d",
    "price"              # Nightly rental price
]

rio_listings = df[target_columns].copy()

rio_listings.sample(5)

rio_listings.describe()

rio_listings.info()

"""tratamento da coluna preço"""

stripped_commas = rio_listings['price'].str.replace(',', '')

stripped_dollars = stripped_commas.str.replace('$', '')

rio_listings['price'] = stripped_dollars.astype('float')

rio_listings.dropna(axis=0, inplace=True)

ordem = rio_listings['property_type'].astype('category').unique().tolist()
rio_listings['property_type_cod'] = pd.Categorical(rio_listings['property_type'], categories=ordem, ordered=True)

rio_listings['property_type_cod'] = rio_listings['property_type_cod'].cat.codes

ordem = rio_listings['neighbourhood_cleansed'].astype('category').unique().tolist()
rio_listings['neighbourhood_cleansed_cod'] = pd.Categorical(rio_listings['neighbourhood_cleansed'], categories=ordem, ordered=True)

rio_listings['neighbourhood_cleansed_cod'] = rio_listings['neighbourhood_cleansed_cod'].cat.codes

ordem = rio_listings['amenities'].astype('category').unique().tolist()
rio_listings['amenities_cod'] = pd.Categorical(rio_listings['amenities'], categories=ordem, ordered=True)

rio_listings['amenities_cod'] = rio_listings['amenities_cod'].cat.codes

rio_listings.sample(5)

rio_listings = rio_listings.drop(['neighbourhood_cleansed', 'property_type', 'amenities'], axis=1)

rio_listings

from matplotlib.figure import Figure

# Funções de outliers
def _iqr_bounds(series: pd.Series, k: float = 1.5):
    q1 = series.quantile(0.25)
    q3 = series.quantile(0.75)
    iqr = q3 - q1
    low = q1 - k * iqr
    up = q3 + k * iqr
    return low, up, {"Q1": q1, "Q3": q3, "IQR": iqr}

def remove_outliers_iqr(df, columns, k=1.5, inclusive=True, dropna=True):
    data = df.copy()
    for c in columns:
        data[c] = pd.to_numeric(data[c], errors="coerce")
    if dropna:
        data = data.dropna(subset=columns)
    before = len(data)
    bounds = {}
    mask = pd.Series(True, index=data.index)
    for c in columns:
        low, up, stats = _iqr_bounds(data[c].dropna(), k=k)
        bounds[c] = {"low": low, "up": up, **stats}
        if inclusive:
            m = (data[c] >= low) & (data[c] <= up)
        else:
            m = (data[c] > low) & (data[c] < up)
        mask &= m
    cleaned = data.loc[mask].copy()
    info = {
        "rows_in": before,
        "rows_out": len(cleaned),
        "rows_removed": before - len(cleaned),
        "k": k,
        "inclusive": inclusive,
        "bounds": bounds,
    }
    return cleaned, info

# Aplica limpeza
rio_iqr_input = rio_listings.select_dtypes(include="number").copy()
rio_iqr, summary = remove_outliers_iqr(rio_iqr_input, list(rio_iqr_input.columns))

# Função 1: Outliers
def aba_outliers():
    texto = f"**Rows before**: {summary['rows_in']}  \n"
    texto += f"**Rows after**: {summary['rows_out']}  \n"
    texto += f"**Removed**: {summary['rows_removed']}  \n\n"
    texto += "**Per-column bounds:**  \n"
    for col, b in summary["bounds"].items():
        texto += f"- `{col}`: [{b['low']:.3f}, {b['up']:.3f}] (Q1={b['Q1']:.3f}, Q3={b['Q3']:.3f}, IQR={b['IQR']:.3f})  \n"
    return texto

# Função 2: Heatmap
def aba_heatmap():
    corr_matrix = rio_iqr.corr()
    fig = plt.figure(figsize=(20, 10))
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", center=0, square=True, cbar_kws={"shrink": 0.75})
    plt.title("Correlation Heatmap of Rio Listings Features", fontsize=16, pad=15)
    plt.xticks(rotation=45, ha="right")
    plt.yticks(rotation=0)
    plt.tight_layout()
    return fig

# Função 3: Estatísticas de price
def aba_descricao_price():
    return rio_iqr["price"].describe().to_frame().to_markdown()

# Função 4: Correlação com price
def aba_correlacao_price():
    price_corr = rio_iqr.corr()["price"].sort_values(ascending=False)
    texto = "**Correlação com `price`:**  \n"
    for col, val in price_corr.items():
        texto += f"- `{col}`: {val:.3f}  \n"
    return texto

# Função 5: Treinamento com PyTorch
def aba_treinamento(n_epochs):
    feature_cols = [c for c in rio_iqr.columns if c != "price"]
    target_col = "price"
    X = rio_iqr[feature_cols].to_numpy(dtype=np.float32)
    y = rio_iqr[target_col].to_numpy(dtype=np.float32).reshape(-1, 1)

    x_tensor = torch.as_tensor(X).float()
    y_tensor = torch.as_tensor(y).float()
    dataset = TensorDataset(x_tensor, y_tensor)

    ratio = 0.8
    n_total = len(dataset)
    n_train = int(n_total * ratio)
    n_val = n_total - n_train
    train_data, val_data = random_split(dataset, [n_train, n_val])

    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=16, shuffle=False)

    class Architecture:
        def __init__(self, model, loss_fn, optimizer):
            self.model = model
            self.loss_fn = loss_fn
            self.optimizer = optimizer
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.model.to(self.device)
            self.train_loader = None
            self.val_loader = None
            self.losses = []
            self.val_losses = []
            self.total_epochs = 0
            self.train_step_fn = self._make_train_step_fn()
            self.val_step_fn = self._make_val_step_fn()

        def set_loaders(self, train_loader, val_loader=None):
            self.train_loader = train_loader
            self.val_loader = val_loader

        def _make_train_step_fn(self):
            def fn(x, y):
                self.model.train()
                yhat = self.model(x)
                loss = self.loss_fn(yhat, y)
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()
                return loss.item()
            return fn

        def _make_val_step_fn(self):
            def fn(x, y):
                self.model.eval()
                yhat = self.model(x)
                loss = self.loss_fn(yhat, y)
                return loss.item()
            return fn

        def _mini_batch(self, validation=False):
            loader = self.val_loader if validation else self.train_loader
            step_fn = self.val_step_fn if validation else self.train_step_fn
            losses = []
            for x, y in loader:
                x, y = x.to(self.device), y.to(self.device)
                losses.append(step_fn(x, y))
            return np.mean(losses)

        def set_seed(self, seed=42):
            torch.manual_seed(seed)
            np.random.seed(seed)

        def train(self, n_epochs, seed=42):
            self.set_seed(seed)
            for _ in range(n_epochs):
                self.total_epochs += 1
                self.losses.append(self._mini_batch(False))
                with torch.no_grad():
                    self.val_losses.append(self._mini_batch(True))

        def plot_losses(self):
          import matplotlib.pyplot as plt
          fig, ax = plt.subplots(figsize=(10, 4))

          if self.losses and self.val_losses:
              ax.plot(self.losses, label='Training Loss', color='blue')
              ax.plot(self.val_losses, label='Validation Loss', color='red')
              ax.set_yscale('log')
          else:
              ax.text(0.5, 0.5, 'Sem dados de perda disponíveis', ha='center', va='center')

          ax.set_xlabel('Epochs')
          ax.set_ylabel('Loss')
          ax.legend()
          fig.tight_layout()
          return fig

    D = x_tensor.shape[1]
    model = nn.Sequential(nn.Linear(D, 1))
    optimizer = optim.SGD(model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()
    arch = Architecture(model, loss_fn, optimizer)
    arch.set_loaders(train_loader, val_loader)
    arch.train(n_epochs=n_epochs)

    return arch.plot_losses()

def aba_lazy():
    X = rio_iqr.drop("price", axis=1)
    y = rio_iqr["price"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    linear_reg_model = LinearRegression()
    linear_reg_model.fit(X_train, y_train)
    y_pred = linear_reg_model.predict(X_test)

    fig = plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred)
    plt.title('Actual vs Predicted Prices (Linear Regression)', fontsize=16)
    plt.xlabel('Actual Price')
    plt.ylabel('Predicted Price')
    plt.grid(True)
    plt.tight_layout()


    return fig


demo = gr.TabbedInterface(
    interface_list=[
        gr.Interface(fn=aba_outliers, inputs=[], outputs=gr.Markdown(), title="Remoção de Outliers"),
        gr.Interface(fn=aba_heatmap, inputs=[], outputs=gr.Plot(), title="Heatmap de Correlação"),
        gr.Interface(fn=aba_descricao_price, inputs=[], outputs=gr.Markdown(), title="Estatísticas"),
        gr.Interface(fn=aba_correlacao_price, inputs=[], outputs=gr.Markdown(), title="Correlação"),
        gr.Interface(
      fn=aba_treinamento,
      inputs=gr.Slider(minimum=1, maximum=150, step=1, label="Número de Épocas"),
      outputs=gr.Plot(),
      title="🧠 Treinamento com PyTorch"
  ),
        gr.Interface(fn=aba_lazy, inputs=[], outputs=[gr.Plot()], title="LazyRegressor + Regressão Linear")
    ],
    tab_names=[  "🔍 Remoção de Outliers",
        "🌡️ Heatmap de Correlação",
        "📊 Estatísticas de Preço",
        "🔗 Correlação com Preço",
        "🧠 Treinamento com PyTorch",
        "⚙️ LazyRegressor + Regressão Linear"],
    title="Regressão Linear aplicada ao Dataset (Rio Listings)"
)

demo.launch(share=True)